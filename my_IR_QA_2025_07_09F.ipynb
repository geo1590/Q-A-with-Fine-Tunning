{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fd426c3319314caca533b4c82ad3a6d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa5d440059b445e4b0d7e3707706549f",
              "IPY_MODEL_647157062b0b45e090e4d816026bb92f",
              "IPY_MODEL_5e79ab970bb44d7f96e5b1c1fb2fe5eb"
            ],
            "layout": "IPY_MODEL_f4f858a4fbb2400e9d33f9cf2bdc32a3"
          }
        },
        "fa5d440059b445e4b0d7e3707706549f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d917c2529bf041ce9e441f5b0c02db2c",
            "placeholder": "​",
            "style": "IPY_MODEL_f61ad55f48f74d9fa7ec00a60e8be9c0",
            "value": "model.safetensors: 100%"
          }
        },
        "647157062b0b45e090e4d816026bb92f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39440094fd5f468d9a570789a85c23cd",
            "max": 265470036,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5451f61f9ed485b93c37615ba79e9be",
            "value": 265470036
          }
        },
        "5e79ab970bb44d7f96e5b1c1fb2fe5eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0338b7b024a4b30bf237ac421585842",
            "placeholder": "​",
            "style": "IPY_MODEL_0783923252b040a0a7908c55fc05dab7",
            "value": " 265M/265M [00:05&lt;00:00, 849kB/s]"
          }
        },
        "f4f858a4fbb2400e9d33f9cf2bdc32a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d917c2529bf041ce9e441f5b0c02db2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f61ad55f48f74d9fa7ec00a60e8be9c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39440094fd5f468d9a570789a85c23cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5451f61f9ed485b93c37615ba79e9be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a0338b7b024a4b30bf237ac421585842": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0783923252b040a0a7908c55fc05dab7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<div style=\"color:#ffffff;\n",
        "          font-size:50px;\n",
        "          font-style:italic;\n",
        "          text-align:left;\n",
        "          font-family: 'Lucida Bright';\n",
        "          background:#4686C8;\">\n",
        "  \t&nbsp; Q&A with Fine-Tuning\n",
        "</div>\n",
        "<br>   \n",
        "<div style=\"\n",
        "          font-size:20px;\n",
        "          text-align:left;\n",
        "          font-family: 'Palatino';\n",
        "          \">\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Project: Q&A using pretrained model with Fine-Tuning<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Author: George Barrinuevo<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Date: 07/09/2025<br>\n",
        "</div>"
      ],
      "metadata": {
        "id": "1GYsQ-_eMAVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><div style=\"color:#ffffff;\n",
        "          font-size:30px;\n",
        "          font-style:italic;\n",
        "          text-align:left;\n",
        "          font-family: 'Lucida Bright';\n",
        "          background:#4686C8;\">\n",
        "  \t      &nbsp; Project Notes\n",
        "</div>\n",
        "<div style=\"\n",
        "          font-size:16px;\n",
        "          text-align:left;\n",
        "          font-family: 'Cambria';\">\n",
        "    \n",
        "<b>My Thoughts</b><br>\n",
        "- This script demonstrates creating a model specifically for Question and Answering tasks. This is for educational purposes only.\n",
        "- A pretrained model that is also already fine-tuned for Question and Answering task is used. Using pretrained models saves a lot of cost and time compared with training the entire model.\n",
        "- The notebook was developed on Google Colab.\n",
        "\n",
        "<b>Technical Details</b><br>\n",
        "<u>Tokenizer for model</u>\n",
        "- The tokenizer is used to data pre-process the text corpus data to a format the model can use. It is highly recommended to use the tokenizer specific to the model being used.\n",
        "\n",
        "<u>Model</u>\n",
        "- A pretrained model is used to save cost and time in training a large model. Later, fine-tuning methods are used to make the model work on specific tasks.\n",
        "- The pretrained model bert-large-uncased-whole-word-masking-finetuned-squad is already fine-tuned to the SQuAD 1.1 dataset, which uses the BertForQuestionAnswering architecture. This architecture specializes in producing an answer based on a question and text.\n",
        "\n",
        "<u>Dataset</u>\n",
        "- SQuAD (Stanford Question Answering Dataset) is a dataset designed for training and evaluating question answering systems. This dataset is download and saved, so it can be reloaded for later use.\n",
        "\n",
        "<u>NLP parser</u>\n",
        "- Spacy is in NLP tokenizer parser. From the input text, it will create the tokenizer, tagger, parser and NER (named entity recogniztion). See https://spacy.io/.\n",
        "\n",
        "<u>Lemmatization</u>\n",
        "- Lemmatization is grouping the various forms of a word. An example is 'walks', 'walking' and 'walked' are part of the base word 'walk'. The 'walk' version is it's lemma.\n",
        "\n",
        "<u>Tokenizing the corpus text</u>\n",
        "- For tokenizing the corpus text (dataset) TF–IDF (term frequency–inverse document frequency) is used. The 'bag of words' disregards word order (and thus most of syntax or grammar), but uses term frequency. An improvement on this is TF-IDF where importance of a word is determined by using 'term frequency'. But for common words uses 'inverse document frequency' where it reduces it's weight if it appears in multiple documents.\n",
        "\n",
        "<u>pipeline process</u>\n",
        "- All of the above is assembled in a pipeline:<br>\n",
        "      SQuAD -> spacy -> lemma -> TF-IDF -> tokenizer -> model -> answer\n"
      ],
      "metadata": {
        "id": "R6fDNaYL-nab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()\n"
      ],
      "metadata": {
        "id": "8QjhjeuwKmSp"
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers sklearn spacy[cuda92]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXSFxN8mNXCu",
        "outputId": "272a53e7-068e-476b-8c58-6cbc1595f220",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Collecting sklearn\n",
            "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import random\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import json\n",
        "import spacy\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering, BertTokenizer, BertForQuestionAnswering\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "9jUpJ40tNGHc"
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "CACHE_DIR = os.getenv('QA_CACHE_PATH', 'cache')\n",
        "DATA_DIR = os.getenv('QA_DATA_PATH', 'data')\n",
        "\n",
        "SQUAD_URL = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\"\n",
        "SQUAD_TRAIN = f\"{DATA_DIR}/train-v2.0.json\"\n",
        "LEMMA_CACHE = f\"{CACHE_DIR}/lemmas.feather\"\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "TOPK = 10 if DEVICE.type == 'cuda' else 5"
      ],
      "metadata": {
        "id": "kgTw35zDPa0b"
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "# os.system(\"jupyter nbextension enable --py widgetsnbextension\")\n",
        "#     The above line is commented out since it causes rendering issues in Github.\n",
        "os.system(\"python3 -m spacy download en_core_web_sm\")\n",
        "\n",
        "if not os.path.isdir(DATA_DIR):\n",
        "    os.mkdir(DATA_DIR)\n",
        "if not os.path.isdir(CACHE_DIR):\n",
        "    os.mkdir(CACHE_DIR)\n",
        "\n",
        "print(\"Downloading pretrained models to cache\")\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', return_token_type_ids=False)\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "if not os.path.isfile(SQUAD_TRAIN):\n",
        "    print(f\"Downloading squad dataset as {SQUAD_TRAIN}\")\n",
        "    response = requests.get(SQUAD_URL, stream=True)\n",
        "\n",
        "    print(f'Saving SQUAD data')\n",
        "    with open(SQUAD_TRAIN, \"wb\") as handle:\n",
        "        for data in tqdm(response.iter_content()):\n",
        "            handle.write(data)\n",
        "\n",
        "print(f'All done')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_lWDdzGNDFV",
        "outputId": "a6e0f028-40cd-4191-a561-f8f964265030"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading pretrained models to cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading squad dataset as data/train-v2.0.json\n",
            "Saving SQUAD data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "42123633it [04:09, 169037.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.prefer_gpu()\n",
        "sp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "0tGQT0DyMAVs"
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "# If there is any issues with loading the below file, than delete the SQUAD_TRAIN directory and re-run.\n",
        "# this notebook. The notebook will download a fresh copy of the file.\n",
        "# This code will load the saved SQuAD data.\n",
        "\n",
        "with open(SQUAD_TRAIN) as f:\n",
        "    doc = json.load(f)\n",
        "doc.keys(), type(doc[\"data\"]), len(doc[\"data\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kd3NyhPFMAVs",
        "outputId": "a1eb3b19-fc61-4147-d73b-dbbf468d753c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(dict_keys(['version', 'data']), list, 442)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracts the paragraphs (context) and questions from the SQuAD dataset.\n",
        "# If there is any issues with loading the below file, than delete the SQUAD_TRAIN directory and re-run this notebook.\n",
        "# The notebook will download a fresh copy of the file and save it for reloading for next script run.\n",
        "\n",
        "paragraphs = []\n",
        "questions = []\n",
        "for topic in doc[\"data\"]:\n",
        "    for pgraph in topic[\"paragraphs\"]:\n",
        "        paragraphs.append(pgraph[\"context\"])\n",
        "        for qa in pgraph[\"qas\"]:\n",
        "            if not qa[\"is_impossible\"]:\n",
        "                questions.append(qa[\"question\"])\n",
        "\n",
        "len(paragraphs), len(questions), random.sample(paragraphs, 2), random.sample(questions, 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-afIYxDTMAVs",
        "outputId": "046e4117-dd37-4a5e-fcfe-6b71130c0b98"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19035,\n",
              " 86821,\n",
              " [\"Comcast Corporation, formerly registered as Comcast Holdings,[note 1] is an American multinational mass media company and is the largest broadcasting and largest cable company in the world by revenue. It is the second largest pay-TV company after the AT&T-DirecTV acquisition, largest cable TV company and largest home Internet service provider in the United States, and the nation's third largest home telephone service provider. Comcast services U.S. residential and commercial customers in 40 states and the District of Columbia. The company's headquarters are located in Philadelphia, Pennsylvania.\",\n",
              "  'Most modern air defence systems are fairly mobile. Even the larger systems tend to be mounted on trailers and are designed to be fairly quickly broken down or set up. In the past, this was not always the case. Early missile systems were cumbersome and required much infrastructure; many could not be moved at all. With the diversification of air defence there has been much more emphasis on mobility. Most modern systems are usually either self-propelled (i.e. guns or missiles are mounted on a truck or tracked chassis) or easily towed. Even systems that consist of many components (transporter/erector/launchers, radars, command posts etc.) benefit from being mounted on a fleet of vehicles. In general, a fixed system can be identified, attacked and destroyed whereas a mobile system can show up in places where it is not expected. Soviet systems especially concentrate on mobility, after the lessons learnt in the Vietnam war between the USA and Vietnam. For more information on this part of the conflict, see SA-2 Guideline.'],\n",
              " ['What can CD parameters be used as references for?',\n",
              "  'The Roman Empire became more controlling of Greece starting in what year?',\n",
              "  'How many middle class were in Egypt in 1950s',\n",
              "  'What does USAAC stand for?',\n",
              "  'What may poor socioeconomic factors ultimately in part be due to?'])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(phrase):\n",
        "    return \" \".join([word.lemma_ for word in sp(phrase)])"
      ],
      "metadata": {
        "id": "76VGHVbvMAVs"
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the lemmatization cache. If it does not exist, then create it by lemmatization of every paragraph.\n",
        "# The 'lemmas' list will contain the lemmatization version of the corresponding 'paragraphs'.\n",
        "# The tqdm() is used to display the progress bar.\n",
        "\n",
        "%%time\n",
        "\n",
        "if not os.path.isfile(LEMMA_CACHE):\n",
        "    lemmas = [lemmatize(par) for par in tqdm(paragraphs)]\n",
        "    df = pd.DataFrame(data={'context': paragraphs, 'lemmas': lemmas})\n",
        "    df.to_feather(LEMMA_CACHE)\n",
        "\n",
        "df = pd.read_feather(LEMMA_CACHE)\n",
        "paragraphs = df.context\n",
        "lemmas = df.lemmas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAL5c3DLMAVs",
        "outputId": "3766a687-cc9b-4273-aa61-2e1e9189e1fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19035/19035 [12:04<00:00, 26.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 11min 15s, sys: 3.87 s, total: 11min 18s\n",
            "Wall time: 12min 4s\n"
          ]
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out an example of a paragraph and lemma.\n",
        "print(f'A paragraph: {paragraphs[1]}')\n",
        "print(f'A lemma: {lemmas[1]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loV9BbGna2g3",
        "outputId": "061b6819-674c-4ab3-f527-52a19d5c1696"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A paragraph: Following the disbandment of Destiny's Child in June 2005, she released her second solo album, B'Day (2006), which contained hits \"Déjà Vu\", \"Irreplaceable\", and \"Beautiful Liar\". Beyoncé also ventured into acting, with a Golden Globe-nominated performance in Dreamgirls (2006), and starring roles in The Pink Panther (2006) and Obsessed (2009). Her marriage to rapper Jay Z and portrayal of Etta James in Cadillac Records (2008) influenced her third album, I Am... Sasha Fierce (2008), which saw the birth of her alter-ego Sasha Fierce and earned a record-setting six Grammy Awards in 2010, including Song of the Year for \"Single Ladies (Put a Ring on It)\". Beyoncé took a hiatus from music in 2010 and took over management of her career; her fourth album 4 (2011) was subsequently mellower in tone, exploring 1970s funk, 1980s pop, and 1990s soul. Her critically acclaimed fifth studio album, Beyoncé (2013), was distinguished from previous releases by its experimental production and exploration of darker themes.\n",
            "A lemma: follow the disbandment of Destiny 's child in June 2005 , she release her second solo album , B'Day ( 2006 ) , which contain hit \" Déjà vu \" , \" irreplaceable \" , and \" Beautiful Liar \" . Beyoncé also venture into acting , with a Golden Globe - nominate performance in Dreamgirls ( 2006 ) , and star role in the Pink Panther ( 2006 ) and Obsessed ( 2009 ) . her marriage to rapper Jay Z and portrayal of Etta James in Cadillac Records ( 2008 ) influence her third album , I be ... Sasha Fierce ( 2008 ) , which see the birth of her alter - ego Sasha Fierce and earn a record - set six Grammy Awards in 2010 , include Song of the Year for \" Single Ladies ( put a ring on it ) \" . Beyoncé take a hiatus from music in 2010 and take over management of her career ; her fourth album 4 ( 2011 ) be subsequently mellower in tone , explore 1970s funk , 1980s pop , and 1990s soul . her critically acclaim fifth studio album , Beyoncé ( 2013 ) , be distinguish from previous release by its experimental production and exploration of dark theme .\n"
          ]
        }
      ],
      "execution_count": 11
    },
    {
      "cell_type": "code",
      "source": [
        "# The code below will load the saved TF-IDF data, but if it does not exist will create this data and save it for later use.\n",
        "# A stop-word are words that have little semantic value and are therefore filtered out.\n",
        "# The code will create a TF-IDF from the Lemmatization of the paragraphs (context) and save this data to be loaded the next time.\n",
        "\n",
        "%%time\n",
        "VECTOR_CACHE = 'cache/vectors.pickle'\n",
        "if not os.path.isfile(VECTOR_CACHE):\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        stop_words='english', min_df=5, max_df=.5, ngram_range=(1,3))\n",
        "    tfidf = vectorizer.fit_transform(lemmas)\n",
        "    with open(VECTOR_CACHE, \"wb\") as f:\n",
        "        pickle.dump(dict(vectorizer=vectorizer, tfidf=tfidf), f)\n",
        "else:\n",
        "    with open(VECTOR_CACHE, \"rb\") as f:\n",
        "        cache = pickle.load(f)\n",
        "        tfidf = cache[\"tfidf\"]\n",
        "        vectorizer = cache[\"vectorizer\"]\n",
        "\n",
        "len(vectorizer.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUpF9wDqMAVt",
        "outputId": "a8089efd-69d6-436f-da64-0ece7718bcc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 8.9 s, sys: 1.52 s, total: 10.4 s\n",
            "Wall time: 10.6 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34552"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": [
        "# This code will create a Lemmatization on the question then converts it to TF-IDF weights.\n",
        "\n",
        "question = \"When did the last country to adopt the Gregorian calendar start using it?\"\n",
        "t_lemmatize = lemmatize(question)\n",
        "query = vectorizer.transform([t_lemmatize])\n",
        "(query > 0).sum(), vectorizer.inverse_transform(query)\n",
        "print(f't_lemmatize: {t_lemmatize}')\n",
        "print(f'query: {query}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_lOP7zsMAVt",
        "outputId": "e17db18b-2443-4897-e0b6-5d117960a1b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t_lemmatize: when do the last country to adopt the gregorian calendar start use it ?\n",
            "query: <Compressed Sparse Row sparse matrix of dtype 'float64'\n",
            "\twith 9 stored elements and shape (1, 34552)>\n",
            "  Coords\tValues\n",
            "  (0, 2372)\t0.24788836762683059\n",
            "  (0, 5792)\t0.34415666630537717\n",
            "  (0, 8571)\t0.17385535976562153\n",
            "  (0, 8573)\t0.4636364467412126\n",
            "  (0, 14446)\t0.3848259593889299\n",
            "  (0, 14447)\t0.4167582282940414\n",
            "  (0, 29795)\t0.20985246114065498\n",
            "  (0, 29812)\t0.43750217041787265\n",
            "  (0, 32566)\t0.1330355626488098\n"
          ]
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": [
        "# This code will calculate the score for query and paragraph matches and sorts them. The 3 highest scoring\n",
        "# paragraphs (context) are displayed.\n",
        "\n",
        "%%time\n",
        "scores = (tfidf * query.T).toarray()\n",
        "results = (np.flip(np.argsort(scores, axis=0)))\n",
        "[paragraphs[i] for i in results[:3, 0]]\n",
        "# paragraphs[results[0]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNJRXcZeMAVt",
        "outputId": "6dd7aa40-6d1a-41f2-b7e5-a7994f115636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 9.59 ms, sys: 9 µs, total: 9.6 ms\n",
            "Wall time: 9.68 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['During the period between 1582, when the first countries adopted the Gregorian calendar, and 1923, when the last European country adopted it, it was often necessary to indicate the date of some event in both the Julian calendar and in the Gregorian calendar, for example, \"10/21 February 1750/51\", where the dual year accounts for some countries already beginning their numbered year on 1 January while others were still using some other date. Even before 1582, the year sometimes had to be double dated because of the different beginnings of the year in various countries. Woolley, writing in his biography of John Dee (1527–1608/9), notes that immediately after 1582 English letter writers \"customarily\" used \"two dates\" on their letters, one OS and one NS.',\n",
              " \"In conjunction with the system of months there is a system of weeks. A physical or electronic calendar provides conversion from a given date to the weekday, and shows multiple dates for a given weekday and month. Calculating the day of the week is not very simple, because of the irregularities in the Gregorian system. When the Gregorian calendar was adopted by each country, the weekly cycle continued uninterrupted. For example, in the case of the few countries that adopted the reformed calendar on the date proposed by Gregory XIII for the calendar's adoption, Friday, 15 October 1582, the preceding date was Thursday, 4 October 1582 (Julian calendar).\",\n",
              " '\"Old Style\" (OS) and \"New Style\" (NS) are sometimes added to dates to identify which system is used in the British Empire and other countries that did not immediately change. Because the Calendar Act of 1750 altered the start of the year, and also aligned the British calendar with the Gregorian calendar, there is some confusion as to what these terms mean. They can indicate that the start of the Julian year has been adjusted to start on 1 January (NS) even though contemporary documents use a different start of year (OS); or to indicate that a date conforms to the Julian calendar (OS), formerly in use in many countries, rather than the Gregorian calendar (NS).']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter for the first 10 highest scoring query to paragraph matches that pass a threshold value and put in a dataframe\n",
        "# format broken down to 'question' and 'context' sections. Store results in a cache file.\n",
        "\n",
        "%%time\n",
        "THRESH = 0.01\n",
        "candidate_idxs = [ (i, scores[i]) for i in results[0:10, 0] ]\n",
        "contexts = [ (paragraphs[i],s)\n",
        "    for (i,s) in candidate_idxs if s > THRESH ]\n",
        "\n",
        "question_df = pd.DataFrame.from_records([ {\n",
        "    'question': question,\n",
        "    'context':  ctx\n",
        "} for (ctx,s) in contexts ])\n",
        "\n",
        "question_df.to_feather(\"cache/question_context.feather\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-0GCtPMMAVt",
        "outputId": "c92d5b9f-8d12-4106-e339-322cbdffc409"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.62 ms, sys: 1.98 ms, total: 5.59 ms\n",
            "Wall time: 6.3 ms\n"
          ]
        }
      ],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "source": [
        "# This implements the Question & Answer Fine-Tunning model using the above data.\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-distilled-squad')\n",
        "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased-distilled-squad')\n",
        "\n",
        "print(f'-----------------')\n",
        "t_record = question_df.to_dict(orient=\"records\")[9]\n",
        "question = t_record['question']\n",
        "text = t_record['context']\n",
        "print(f'\\nquestion: {question}')\n",
        "print(f'\\ntext: {text}')\n",
        "\n",
        "inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "answer_start_index = torch.argmax(outputs.start_logits)\n",
        "answer_end_index = torch.argmax(outputs.end_logits)\n",
        "\n",
        "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "t_answer = tokenizer.decode(predict_answer_tokens)\n",
        "print(f'\\nanswer: {t_answer}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191,
          "referenced_widgets": [
            "fd426c3319314caca533b4c82ad3a6d6",
            "fa5d440059b445e4b0d7e3707706549f",
            "647157062b0b45e090e4d816026bb92f",
            "5e79ab970bb44d7f96e5b1c1fb2fe5eb",
            "f4f858a4fbb2400e9d33f9cf2bdc32a3",
            "d917c2529bf041ce9e441f5b0c02db2c",
            "f61ad55f48f74d9fa7ec00a60e8be9c0",
            "39440094fd5f468d9a570789a85c23cd",
            "c5451f61f9ed485b93c37615ba79e9be",
            "a0338b7b024a4b30bf237ac421585842",
            "0783923252b040a0a7908c55fc05dab7"
          ]
        },
        "id": "ypgO8qthljd-",
        "outputId": "2ea79f01-9352-4e84-e9a1-5697bb0914f8",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd426c3319314caca533b4c82ad3a6d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------\n",
            "\n",
            "question: When did the last country to adopt the Gregorian calendar start using it?\n",
            "\n",
            "text: Philip II of Spain decreed the change from the Julian to the Gregorian calendar, which affected much of Roman Catholic Europe, as Philip was at the time ruler over Spain and Portugal as well as much of Italy. In these territories, as well as in the Polish–Lithuanian Commonwealth (ruled by Anna Jagiellon) and in the Papal States, the new calendar was implemented on the date specified by the bull, with Julian Thursday, 4 October 1582, being followed by Gregorian Friday, 15 October 1582. The Spanish and Portuguese colonies followed somewhat later de facto because of delay in communication.\n",
            "\n",
            "answer: 15 october 1582\n"
          ]
        }
      ],
      "execution_count": 16
    }
  ]
}
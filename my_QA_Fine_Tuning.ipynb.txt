{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "colab": {
      "provenance": []
    },
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<div style=\"color:#ffffff;\n",
        "          font-size:50px;\n",
        "          font-style:italic;\n",
        "          text-align:left;\n",
        "          font-family: 'Lucida Bright';\n",
        "          background:#4686C8;\">\n",
        "  \t&nbsp; Q&A with Fine-Tuning\n",
        "</div>\n",
        "<br>   \n",
        "<div style=\"\n",
        "          font-size:20px;\n",
        "          text-align:left;\n",
        "          font-family: 'Palatino';\n",
        "          \">\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Project: Q&A using pretrained model with Fine-Tuning<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Author: George Barrinuevo<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Date: 07/09/2025<br>\n",
        "</div>"
      ],
      "metadata": {
        "id": "1GYsQ-_eMAVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><div style=\"color:#ffffff;\n",
        "          font-size:30px;\n",
        "          font-style:italic;\n",
        "          text-align:left;\n",
        "          font-family: 'Lucida Bright';\n",
        "          background:#4686C8;\">\n",
        "  \t      &nbsp; Project Notes\n",
        "</div>\n",
        "<div style=\"\n",
        "          font-size:16px;\n",
        "          text-align:left;\n",
        "          font-family: 'Cambria';\">\n",
        "    \n",
        "<b>My Thoughts</b><br>\n",
        "- This script demonstrates creating a model specifically for Question and Answering tasks. This is for educational purposes only.\n",
        "- A pretrained model that is also already fine-tuned for Question and Answering task is used. Using pretrained models saves a lot of cost and time compared with training the entire model.\n",
        "- The notebook was developed on Google Colab.\n",
        "\n",
        "<b>Technical Details</b><br>\n",
        "<u>Tokenizer for model</u>\n",
        "- The tokenizer is used to data pre-process the text corpus data to a format the model can use. It is highly recommended to use the tokenizer specific to the model being used.\n",
        "\n",
        "<u>Model</u>\n",
        "- A pretrained model is used to save cost and time in training a large model. Later, fine-tuning methods are used to make the model work on specific tasks.\n",
        "- The pretrained model bert-large-uncased-whole-word-masking-finetuned-squad is already fine-tuned to the SQuAD 1.1 dataset, which uses the BertForQuestionAnswering architecture. This architecture specializes in producing an answer based on a question and text.\n",
        "\n",
        "<u>Dataset</u>\n",
        "- SQuAD (Stanford Question Answering Dataset) is a dataset designed for training and evaluating question answering systems. This dataset is download and saved, so it can be reloaded for later use.\n",
        "\n",
        "<u>NLP parser</u>\n",
        "- Spacy is in NLP tokenizer parser. From the input text, it will create the tokenizer, tagger, parser and NER (named entity recogniztion). See https://spacy.io/.\n",
        "\n",
        "<u>Lemmatization</u>\n",
        "- Lemmatization is grouping the various forms of a word. An example is 'walks', 'walking' and 'walked' are part of the base word 'walk'. The 'walk' version is it's lemma.\n",
        "\n",
        "<u>Tokenizing the corpus text</u>\n",
        "- For tokenizing the corpus text (dataset) TF–IDF (term frequency–inverse document frequency) is used. The 'bag of words' disregards word order (and thus most of syntax or grammar), but uses term frequency. An improvement on this is TF-IDF where importance of a word is determined by using 'term frequency'. But for common words uses 'inverse document frequency' where it reduces it's weight if it appears in multiple documents.\n",
        "\n",
        "<u>pipeline process</u>\n",
        "- All of the above is assembled in a pipeline:<br>\n",
        "      SQuAD -> spacy -> lemma -> TF-IDF -> tokenizer -> model -> answer\n"
      ],
      "metadata": {
        "id": "R6fDNaYL-nab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n"
      ],
      "metadata": {
        "id": "8QjhjeuwKmSp"
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers sklearn spacy[cuda92]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXSFxN8mNXCu",
        "outputId": "300b0ab9-c537-4d31-d8e0-cedd610b0db2",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Collecting sklearn\n",
            "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import random\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import json\n",
        "import spacy\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering, BertTokenizer, BertForQuestionAnswering\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "9jUpJ40tNGHc"
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "CACHE_DIR = os.getenv('QA_CACHE_PATH', 'cache')\n",
        "DATA_DIR = os.getenv('QA_DATA_PATH', 'data')\n",
        "\n",
        "SQUAD_URL = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\"\n",
        "SQUAD_TRAIN = f\"{DATA_DIR}/train-v2.0.json\"\n",
        "LEMMA_CACHE = f\"{CACHE_DIR}/lemmas.feather\"\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "TOPK = 10 if DEVICE.type == 'cuda' else 5"
      ],
      "metadata": {
        "id": "kgTw35zDPa0b"
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "os.system(\"jupyter nbextension enable --py widgetsnbextension\")\n",
        "os.system(\"python3 -m spacy download en_core_web_sm\")\n",
        "\n",
        "if not os.path.isdir(DATA_DIR):\n",
        "    os.mkdir(DATA_DIR)\n",
        "if not os.path.isdir(CACHE_DIR):\n",
        "    os.mkdir(CACHE_DIR)\n",
        "\n",
        "print(\"Downloading pretrained models to cache\")\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', return_token_type_ids=False)\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "if not os.path.isfile(SQUAD_TRAIN):\n",
        "    print(f\"Downloading squad dataset as {SQUAD_TRAIN}\")\n",
        "    response = requests.get(SQUAD_URL, stream=True)\n",
        "\n",
        "    print(f'Saving SQUAD data')\n",
        "    with open(SQUAD_TRAIN, \"wb\") as handle:\n",
        "        for data in tqdm(response.iter_content()):\n",
        "            handle.write(data)\n",
        "\n",
        "print(f'All done')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455,
          "referenced_widgets": [
            "ac1e5f669c504979bde63aa392dc2258",
            "a79cf842370a492fb0b03e3fdd523bc6",
            "aeb1921e9d8d4101a5723697dafe6ce3",
            "de24efc4329c4567b3442addfc74e571",
            "21256273da0443dd914fb3353cf14b4e",
            "c0e8fda7573b4f83af6256c5a3706731",
            "438b7825d997412f8925b753dee6a92d",
            "ce8b2013b859480ea604b120cd6cb792",
            "f2cf84f313d949ad823f8659f46c63f6",
            "0199e9f587d24ecdb097385914af0ead",
            "7f4a3eef6afb46aa9315363dd17b9cfd",
            "ea900283d42f4871b3ea069fe2d7b61c",
            "936659bcde234c09905a0ddf25899dba",
            "fd2c58332bfd451db5bcb37f05aa9431",
            "7ba7512a085d49a0bce2ebf5a917c53e",
            "27fd817ac77d4dd6ac257eaae2dfd807",
            "8c8b5408dae74d20a55e9bf0b6019a7a",
            "7997ca7103a24533b630927783e46d50",
            "e2015fb527bd4fadbb31269e349f77df",
            "b90f95610fd24a52a81130d91f41894d",
            "f18338ee36674302a4c1e1e1257fdd24",
            "50fd4af604d74c81bbf8e728ab5f3280",
            "5d79e57a80d245b6be408fdc51708def",
            "c405acf8c9b8471ab7e9b9fbaee23cd4",
            "b9f5cace72ea4e1898cd0174beaf95ab",
            "d4540038ced34e839d3647435acb8ffe",
            "395fa819d93946b2a897562add63e0d9",
            "6c2c233d61a649648cc498c9014c327e",
            "fd26c8591c1044688fe9489cbf8cf0fd",
            "33193c48ca84418dabe511fe206a7c47",
            "de935f222c2b43d5bf944fa3d68aa894",
            "c554215887e84b9aa51afef7b54fc19a",
            "ec0939b47cb74efd8f90ae4cdc7d6e4f",
            "41dddd1cbf704c4e877e95787a7aff18",
            "fe00008588f740b49a3cd3d7f11abac4",
            "6fd33770e40c408d8035c616b9236016",
            "a949c1b92b194ddabfeb5490bb82cef5",
            "35349b56529e4605b7ba2e121b1f2112",
            "81b8f7fc3a1e4d36aa4f5f2e75b38b04",
            "7b218ebc9537409f801b3dbba7af49fc",
            "3841467f089d4960929a799155d78a9e",
            "3b726c19c7004260a3e041dcc7b821bb",
            "68204820dbca4af28f7f3f9fc73fb49b",
            "413635232bf0411c94737db98ff0715f",
            "fd2e827d2dfb4dd39320e6e6cb737bdb",
            "74699e39c9f5438db0bde894761df671",
            "7d15d9e8d4a8455ea726b4025b7420a8",
            "7ba1bb835c834b94a6b157a9fbe0ee25",
            "ca869e9a95c1467b8fedc3be82ba8049",
            "286fb49592024850b0651d9b75b04942",
            "9fae0f441b2e4d618fedb093fb216bc0",
            "95eedb2d7df4479e8843130641fdf34d",
            "e609225b35fb464990b9539445ba2e37",
            "89ee75aaf7db49ec84ce079e8e0bb94a",
            "566b5bfc669b4ef4b8cc6c7d040aa8fd",
            "208a57a85ef0442ca41c94e131526d10",
            "7f81496dbd624ca1a8ed19b3b92e0bc3",
            "c16a2f6c9b034e7ea93d9d7a048e89e8",
            "dd78a4959b73427c9100719ca2a25aec",
            "7d841671c16844acb030735c222f2d76",
            "af9eeddff46544bf84940711073ab64b",
            "b4b2a3dbb6ca41deb0631ce7a59aefae",
            "0977b650a2c441dfacb98dbe0dcf8479",
            "6e84f0cfb2e04458882d3d8a17a618a3",
            "be311318c5e743aa976baa8e39a74bef",
            "fa71f87f38434458b3666ee19cb4819b"
          ]
        },
        "id": "w_lWDdzGNDFV",
        "outputId": "4c9116bf-9f1d-4a14-eb98-2edf5d957ce5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading pretrained models to cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac1e5f669c504979bde63aa392dc2258"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea900283d42f4871b3ea069fe2d7b61c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d79e57a80d245b6be408fdc51708def"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41dddd1cbf704c4e877e95787a7aff18"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd2e827d2dfb4dd39320e6e6cb737bdb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "208a57a85ef0442ca41c94e131526d10"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading squad dataset as data/train-v2.0.json\n",
            "Saving SQUAD data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "42123633it [03:03, 229751.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.prefer_gpu()\n",
        "sp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "0tGQT0DyMAVs"
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "# If there is any issues with loading the below file, than delete the SQUAD_TRAIN directory and re-run.\n",
        "# this notebook. The notebook will download a fresh copy of the file.\n",
        "# This code will load the saved SQuAD data.\n",
        "\n",
        "with open(SQUAD_TRAIN) as f:\n",
        "    doc = json.load(f)\n",
        "doc.keys(), type(doc[\"data\"]), len(doc[\"data\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kd3NyhPFMAVs",
        "outputId": "bc927481-6c75-4201-f25f-0765e01d26c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(dict_keys(['version', 'data']), list, 442)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracts the paragraphs (context) and questions from the SQuAD dataset.\n",
        "# If there is any issues with loading the below file, than delete the SQUAD_TRAIN directory and re-run this notebook.\n",
        "# The notebook will download a fresh copy of the file and save it for reloading for next script run.\n",
        "\n",
        "paragraphs = []\n",
        "questions = []\n",
        "for topic in doc[\"data\"]:\n",
        "    for pgraph in topic[\"paragraphs\"]:\n",
        "        paragraphs.append(pgraph[\"context\"])\n",
        "        for qa in pgraph[\"qas\"]:\n",
        "            if not qa[\"is_impossible\"]:\n",
        "                questions.append(qa[\"question\"])\n",
        "\n",
        "len(paragraphs), len(questions), random.sample(paragraphs, 2), random.sample(questions, 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-afIYxDTMAVs",
        "outputId": "e4c495b1-d812-437f-9067-b3190c34dd24"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19035,\n",
              " 86821,\n",
              " ['Many causes for the financial crisis have been suggested, with varying weight assigned by experts. The U.S. Senate\\'s Levin–Coburn Report concluded that the crisis was the result of \"high risk, complex financial products; undisclosed conflicts of interest; the failure of regulators, the credit rating agencies, and the market itself to rein in the excesses of Wall Street.\" The Financial Crisis Inquiry Commission concluded that the financial crisis was avoidable and was caused by \"widespread failures in financial regulation and supervision\", \"dramatic failures of corporate governance and risk management at many systemically important financial institutions\", \"a combination of excessive borrowing, risky investments, and lack of transparency\" by financial institutions, ill preparation and inconsistent action by government that \"added to the uncertainty and panic\", a \"systemic breakdown in accountability and ethics\", \"collapsing mortgage-lending standards and the mortgage securitization pipeline\", deregulation of over-the-counter derivatives, especially credit default swaps, and \"the failures of credit rating agencies\" to correctly price risk. The 1999 repeal of the Glass-Steagall Act effectively removed the separation between investment banks and depository banks in the United States. Critics argued that credit rating agencies and investors failed to accurately price the risk involved with mortgage-related financial products, and that governments did not adjust their regulatory practices to address 21st-century financial markets. Research into the causes of the financial crisis has also focused on the role of interest rate spreads.',\n",
              "  'Violent incidents occurred throughout the Piedmont of the state as white insurgents struggled to maintain white supremacy in the face of social changes after the war and granting of citizenship to freedmen by federal constitutional amendments. After former Confederates were allowed to vote again, election campaigns from 1872 on were marked by violent intimidation of blacks and Republicans by white Democratic paramilitary groups, known as the Red Shirts. Violent incidents took place in Charleston on King Street in September 6 and in nearby Cainhoy on October 15, both in association with political meetings before the 1876 election. The Cainhoy incident was the only one statewide in which more whites were killed than blacks. The Red Shirts were instrumental in suppressing the black Republican vote in some areas in 1876 and narrowly electing Wade Hampton as governor, and taking back control of the state legislature. Another riot occurred in Charleston the day after the election, when a prominent Republican leader was mistakenly reported killed.'],\n",
              " ['What was the former name of UPS Freight?',\n",
              "  'The Solar Total Energy Project had a field of how many parabolic dishes?',\n",
              "  'What document governs the defense of the Marshall Islands?',\n",
              "  'When did the French authorities adopted the pro-European Unification position? ',\n",
              "  'The behavior of which two phenomena caused physicists to encounter problems with their understanding of time in the late 19th century?'])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(phrase):\n",
        "    return \" \".join([word.lemma_ for word in sp(phrase)])"
      ],
      "metadata": {
        "id": "76VGHVbvMAVs"
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the lemmatization cache. If it does not exist, then create it by lemmatization of every paragraph.\n",
        "# The 'lemmas' list will contain the lemmatization version of the corresponding 'paragraphs'.\n",
        "# The tqdm() is used to display the progress bar.\n",
        "\n",
        "%%time\n",
        "\n",
        "if not os.path.isfile(LEMMA_CACHE):\n",
        "    lemmas = [lemmatize(par) for par in tqdm(paragraphs)]\n",
        "    df = pd.DataFrame(data={'context': paragraphs, 'lemmas': lemmas})\n",
        "    df.to_feather(LEMMA_CACHE)\n",
        "\n",
        "df = pd.read_feather(LEMMA_CACHE)\n",
        "paragraphs = df.context\n",
        "lemmas = df.lemmas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAL5c3DLMAVs",
        "outputId": "bdc53a62-0450-4e97-bc69-f3ce2f672d57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19035/19035 [10:14<00:00, 30.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 9min 36s, sys: 3.01 s, total: 9min 39s\n",
            "Wall time: 10min 14s\n"
          ]
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out an example of a paragraph and lemma.\n",
        "print(f'A paragraph: {paragraphs[1]}')\n",
        "print(f'A lemma: {lemmas[1]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loV9BbGna2g3",
        "outputId": "9842e26f-402e-4857-8ead-b739ab964a8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A paragraph: Following the disbandment of Destiny's Child in June 2005, she released her second solo album, B'Day (2006), which contained hits \"Déjà Vu\", \"Irreplaceable\", and \"Beautiful Liar\". Beyoncé also ventured into acting, with a Golden Globe-nominated performance in Dreamgirls (2006), and starring roles in The Pink Panther (2006) and Obsessed (2009). Her marriage to rapper Jay Z and portrayal of Etta James in Cadillac Records (2008) influenced her third album, I Am... Sasha Fierce (2008), which saw the birth of her alter-ego Sasha Fierce and earned a record-setting six Grammy Awards in 2010, including Song of the Year for \"Single Ladies (Put a Ring on It)\". Beyoncé took a hiatus from music in 2010 and took over management of her career; her fourth album 4 (2011) was subsequently mellower in tone, exploring 1970s funk, 1980s pop, and 1990s soul. Her critically acclaimed fifth studio album, Beyoncé (2013), was distinguished from previous releases by its experimental production and exploration of darker themes.\n",
            "A lemma: follow the disbandment of Destiny 's child in June 2005 , she release her second solo album , B'Day ( 2006 ) , which contain hit \" Déjà vu \" , \" irreplaceable \" , and \" Beautiful Liar \" . Beyoncé also venture into acting , with a Golden Globe - nominate performance in Dreamgirls ( 2006 ) , and star role in the Pink Panther ( 2006 ) and Obsessed ( 2009 ) . her marriage to rapper Jay Z and portrayal of Etta James in Cadillac Records ( 2008 ) influence her third album , I be ... Sasha Fierce ( 2008 ) , which see the birth of her alter - ego Sasha Fierce and earn a record - set six Grammy Awards in 2010 , include Song of the Year for \" Single Ladies ( put a ring on it ) \" . Beyoncé take a hiatus from music in 2010 and take over management of her career ; her fourth album 4 ( 2011 ) be subsequently mellower in tone , explore 1970s funk , 1980s pop , and 1990s soul . her critically acclaim fifth studio album , Beyoncé ( 2013 ) , be distinguish from previous release by its experimental production and exploration of dark theme .\n"
          ]
        }
      ],
      "execution_count": 11
    },
    {
      "cell_type": "code",
      "source": [
        "# The code below will load the saved TF-IDF data, but if it does not exist will create this data and save it for later use.\n",
        "# A stop-word are words that have little semantic value and are therefore filtered out.\n",
        "# The code will create a TF-IDF from the Lemmatization of the paragraphs (context) and save this data to be loaded the next time.\n",
        "\n",
        "%%time\n",
        "VECTOR_CACHE = 'cache/vectors.pickle'\n",
        "if not os.path.isfile(VECTOR_CACHE):\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        stop_words='english', min_df=5, max_df=.5, ngram_range=(1,3))\n",
        "    tfidf = vectorizer.fit_transform(lemmas)\n",
        "    with open(VECTOR_CACHE, \"wb\") as f:\n",
        "        pickle.dump(dict(vectorizer=vectorizer, tfidf=tfidf), f)\n",
        "else:\n",
        "    with open(VECTOR_CACHE, \"rb\") as f:\n",
        "        cache = pickle.load(f)\n",
        "        tfidf = cache[\"tfidf\"]\n",
        "        vectorizer = cache[\"vectorizer\"]\n",
        "\n",
        "len(vectorizer.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUpF9wDqMAVt",
        "outputId": "2cf8b574-6488-418f-a1e9-d6764e89e3aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 6.26 s, sys: 1.58 s, total: 7.85 s\n",
            "Wall time: 7.88 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34552"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": [
        "# This code will create a Lemmatization on the question then converts it to TF-IDF weights.\n",
        "\n",
        "question = \"When did the last country to adopt the Gregorian calendar start using it?\"\n",
        "t_lemmatize = lemmatize(question)\n",
        "query = vectorizer.transform([t_lemmatize])\n",
        "(query > 0).sum(), vectorizer.inverse_transform(query)\n",
        "print(f't_lemmatize: {t_lemmatize}')\n",
        "print(f'query: {query}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_lOP7zsMAVt",
        "outputId": "873c71fb-080a-44f6-bb7e-db0322d06ed4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t_lemmatize: when do the last country to adopt the gregorian calendar start use it ?\n",
            "query: <Compressed Sparse Row sparse matrix of dtype 'float64'\n",
            "\twith 9 stored elements and shape (1, 34552)>\n",
            "  Coords\tValues\n",
            "  (0, 2372)\t0.24788836762683059\n",
            "  (0, 5792)\t0.34415666630537717\n",
            "  (0, 8571)\t0.17385535976562153\n",
            "  (0, 8573)\t0.4636364467412126\n",
            "  (0, 14446)\t0.3848259593889299\n",
            "  (0, 14447)\t0.4167582282940414\n",
            "  (0, 29795)\t0.20985246114065498\n",
            "  (0, 29812)\t0.43750217041787265\n",
            "  (0, 32566)\t0.1330355626488098\n"
          ]
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": [
        "# This code will calculate the score for query and paragraph matches and sorts them. The 3 highest scoring\n",
        "# paragraphs (context) are displayed.\n",
        "\n",
        "%%time\n",
        "scores = (tfidf * query.T).toarray()\n",
        "results = (np.flip(np.argsort(scores, axis=0)))\n",
        "[paragraphs[i] for i in results[:3, 0]]\n",
        "# paragraphs[results[0]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNJRXcZeMAVt",
        "outputId": "c2e05c37-433c-4cb0-f3f8-3cadc95ac4c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 9.23 ms, sys: 6 µs, total: 9.24 ms\n",
            "Wall time: 9.38 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['During the period between 1582, when the first countries adopted the Gregorian calendar, and 1923, when the last European country adopted it, it was often necessary to indicate the date of some event in both the Julian calendar and in the Gregorian calendar, for example, \"10/21 February 1750/51\", where the dual year accounts for some countries already beginning their numbered year on 1 January while others were still using some other date. Even before 1582, the year sometimes had to be double dated because of the different beginnings of the year in various countries. Woolley, writing in his biography of John Dee (1527–1608/9), notes that immediately after 1582 English letter writers \"customarily\" used \"two dates\" on their letters, one OS and one NS.',\n",
              " \"In conjunction with the system of months there is a system of weeks. A physical or electronic calendar provides conversion from a given date to the weekday, and shows multiple dates for a given weekday and month. Calculating the day of the week is not very simple, because of the irregularities in the Gregorian system. When the Gregorian calendar was adopted by each country, the weekly cycle continued uninterrupted. For example, in the case of the few countries that adopted the reformed calendar on the date proposed by Gregory XIII for the calendar's adoption, Friday, 15 October 1582, the preceding date was Thursday, 4 October 1582 (Julian calendar).\",\n",
              " '\"Old Style\" (OS) and \"New Style\" (NS) are sometimes added to dates to identify which system is used in the British Empire and other countries that did not immediately change. Because the Calendar Act of 1750 altered the start of the year, and also aligned the British calendar with the Gregorian calendar, there is some confusion as to what these terms mean. They can indicate that the start of the Julian year has been adjusted to start on 1 January (NS) even though contemporary documents use a different start of year (OS); or to indicate that a date conforms to the Julian calendar (OS), formerly in use in many countries, rather than the Gregorian calendar (NS).']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter for the first 10 highest scoring query to paragraph matches that pass a threshold value and put in a dataframe\n",
        "# format broken down to 'question' and 'context' sections. Store results in a cache file.\n",
        "\n",
        "%%time\n",
        "THRESH = 0.01\n",
        "candidate_idxs = [ (i, scores[i]) for i in results[0:10, 0] ]\n",
        "contexts = [ (paragraphs[i],s)\n",
        "    for (i,s) in candidate_idxs if s > THRESH ]\n",
        "\n",
        "question_df = pd.DataFrame.from_records([ {\n",
        "    'question': question,\n",
        "    'context':  ctx\n",
        "} for (ctx,s) in contexts ])\n",
        "\n",
        "question_df.to_feather(\"cache/question_context.feather\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-0GCtPMMAVt",
        "outputId": "6ff44e01-94bc-44b2-e181-d728d304290a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.05 ms, sys: 984 µs, total: 4.04 ms\n",
            "Wall time: 6.02 ms\n"
          ]
        }
      ],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "source": [
        "# This implements the Question & Answer Fine-Tunning model using the above data.\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-distilled-squad')\n",
        "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased-distilled-squad')\n",
        "\n",
        "print(f'-----------------')\n",
        "t_record = question_df.to_dict(orient=\"records\")[9]\n",
        "question = t_record['question']\n",
        "text = t_record['context']\n",
        "print(f'\\nquestion: {question}')\n",
        "print(f'\\ntext: {text}')\n",
        "\n",
        "inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "answer_start_index = torch.argmax(outputs.start_logits)\n",
        "answer_end_index = torch.argmax(outputs.end_logits)\n",
        "\n",
        "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "t_answer = tokenizer.decode(predict_answer_tokens)\n",
        "print(f'\\nanswer: {t_answer}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319,
          "referenced_widgets": [
            "e88aae551a544ee0b7cab4418980558b",
            "59b09ea37e5a41d790b4e03b313cd50a",
            "7fdb10a12c774e0189b3dfa06364db10",
            "1a3600332acc4cb29a5d5e8f69b19fa5",
            "caee572e767040b6bb9f9470254b32f5",
            "cbc29813c18c4800a4a48a1a54120b79",
            "9215778a690d4a559707c9e0b06a2b7a",
            "58adc8d8af2d417f8d46e401f255decc",
            "9a683d50f87b4e359c1eb4a7e0507400",
            "f80b2ab091544c08b93f8ef992ddd86a",
            "5c63bfaff21b4dcca79ad29e6734a6c2",
            "e1121bcd6e084820879b241290bd660f",
            "a6e750de6d784811afbfaf91b63277f9",
            "4d5ea3dbeed94bda8821924eacebac4b",
            "9ebf3f1634a64ee5b725ec3c6881da3f",
            "4f62c9e80b5849f7bba24d15c2f33523",
            "c17d95c1943b49f887002b14fa59c674",
            "f6994ead60884d75847aa9692abd9276",
            "3218b59916544fd194b3d8e75523ee4d",
            "e1a732716d7a4e418723bd86e3ba812d",
            "cd077d8cd11d4558a159f4d4b58bd672",
            "1173bd3d447e4ae7b666f19d121d0e43",
            "448fd258da364cff8c92ea8f6c503ced",
            "cebfaf9e2d0f4c9c903c2278bf2b3f9e",
            "28bdbfccaba54541b636237ee43a06e6",
            "0a22a20472f3456387c886ee3dd3c8cb",
            "639306afd91c4b77be1f152882faa664",
            "eabbfa826eb34a87995847deb6fb22e8",
            "c4c95a9cda084857babc5296dbd484a7",
            "1a176745630441d1a17de1ec1ddc8944",
            "e53be6866110467e9e40365b95975c43",
            "befacf85710646c292c0f8bee82f7012",
            "3b4f04eb4622421daa552a0dcc127b74",
            "4d8293c84a144f3d9b2fea8b8a7f1ad8",
            "caaca5ad156542ee9e0b99c96aa4442a",
            "ebaa6ca81b6e454da42cee33671e607f",
            "c3885c579eae4e139193266b946018fa",
            "7b89e2e0b70f43cdb2e1a8805ca09d26",
            "a4f756dbe2e34de89d4194fa875b684d",
            "9ff474e150fb4166abb072134aa444ad",
            "8c788e3d776d4cd196430047d38c3877",
            "d0409a3581b74e52a8c44afb29e9fd42",
            "f856d459691047bfa60442ef84a66ab3",
            "b4b88d7d340b4e7cbd53d2507244c245",
            "cf38761d2e0141beb3bad3145d972234",
            "cbf01305eacd422a9342d44d22ec1420",
            "23e1f87f4f774344adba44b779953557",
            "db4a54ba130b4cfca432635c13c9c054",
            "2d610c05b31149db89bb120c2cbcbe52",
            "289b9e53a0764e1f829a53cfd637411c",
            "27a4c481152a4c64a2e6ea6d22da6f32",
            "d6bb6d936dac4b119713ed614fd57687",
            "2edd829315b04bf8aa86dec149432bae",
            "5afe6f5d2a97447fb51ef9546ec11c72",
            "5f90c7a15c6842ba807db0d009a529a4"
          ]
        },
        "id": "ypgO8qthljd-",
        "outputId": "4770e831-3933-43a0-e07a-8445faaf3a39",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e88aae551a544ee0b7cab4418980558b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1121bcd6e084820879b241290bd660f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "448fd258da364cff8c92ea8f6c503ced"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/451 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d8293c84a144f3d9b2fea8b8a7f1ad8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf38761d2e0141beb3bad3145d972234"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------\n",
            "\n",
            "question: When did the last country to adopt the Gregorian calendar start using it?\n",
            "\n",
            "text: Philip II of Spain decreed the change from the Julian to the Gregorian calendar, which affected much of Roman Catholic Europe, as Philip was at the time ruler over Spain and Portugal as well as much of Italy. In these territories, as well as in the Polish–Lithuanian Commonwealth (ruled by Anna Jagiellon) and in the Papal States, the new calendar was implemented on the date specified by the bull, with Julian Thursday, 4 October 1582, being followed by Gregorian Friday, 15 October 1582. The Spanish and Portuguese colonies followed somewhat later de facto because of delay in communication.\n",
            "\n",
            "answer: 15 october 1582\n"
          ]
        }
      ],
      "execution_count": 16
    }
  ]
}